{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e189af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/huixian/.conda/envs/multiood/lib/python3.12/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'map' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    161\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n\u001b[0;32m--> 162\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_dataset))\n\u001b[1;32m    164\u001b[0m test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_dataset) \u001b[38;5;241m-\u001b[39m train_size \u001b[38;5;241m-\u001b[39m val_size\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mVideoClipDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'map' has no len()"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpu_ids = [4]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- SETTINGS ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/Clips_16frames\"\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/clip_sentiment_mapping_final.csv\"\n",
    "\n",
    "negative_samples = 1500\n",
    "neutral_samples = 2000\n",
    "positive_samples = 1500\n",
    "batch_size = 16\n",
    "clip_len = 16\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- DATASET ----\n",
    "# class VideoClipDataset(Dataset):\n",
    "#     def __init__(self, clip_dir, csv_path, feature_extractor):\n",
    "#         self.clip_dir = clip_dir\n",
    "#         self.df = pd.read_csv(csv_path)\n",
    "#         self.feature_extractor = feature_extractor\n",
    "\n",
    "#         # Group by sentiment label\n",
    "#         grouped = self.df.groupby(\"sentiment_label\")\n",
    "\n",
    "#         self.samples = []\n",
    "#         for label, n_samples in zip([\"Negative\", \"Neutral\", \"Positive\"], [negative_samples, neutral_samples, positive_samples]):\n",
    "#             group = grouped.get_group(label)\n",
    "#             if label == \"Negative\":\n",
    "#                 sorted_group = group.sort_values(\"sentiment_score\")\n",
    "#             elif label == \"Neutral\":\n",
    "#                 sorted_group = group.reindex((group[\"sentiment_score\"] - 0).abs().sort_values().index)\n",
    "#             else:  # Positive\n",
    "#                 sorted_group = group.sort_values(\"sentiment_score\", ascending=False)\n",
    "\n",
    "#             selected = sorted_group.head(n_samples)\n",
    "#             self.samples.extend(selected.itertuples(index=False))\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, clip_dir, csv_path, feature_extractor):\n",
    "        self.clip_dir = clip_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        # Use all samples directly\n",
    "        self.samples = self.df.itertuples(index=False)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.samples[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row.clip_filename)\n",
    "\n",
    "        cap = cv2.VideoCapture(clip_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame[:, :, ::-1])  # BGR to RGB\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < clip_len:\n",
    "            frames += [frames[-1]] * (clip_len - len(frames))\n",
    "        frames = frames[:clip_len]\n",
    "\n",
    "        inputs = self.feature_extractor(images=frames, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        return inputs, torch.tensor(row.sentiment_score, dtype=torch.float32)\n",
    "\n",
    "# ---- LOSS ----\n",
    "class CenteredWeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        ideal = torch.zeros_like(targets)\n",
    "        ideal[targets < -0.3] = -3.0\n",
    "        ideal[targets > 0.3] = 3.0\n",
    "        ideal[(-0.3 <= targets) & (targets <= 0.3)] = 0.0\n",
    "\n",
    "        weights = torch.ones_like(targets)\n",
    "        weights[targets < -0.3] = 2.0\n",
    "        weights[targets > 0.3] = 2.0\n",
    "        weights[(-0.3 <= targets) & (targets <= 0.3)] = 1.0\n",
    "\n",
    "        mse = (preds - ideal) ** 2\n",
    "        return (weights * mse).mean()\n",
    "\n",
    "# ---- MODEL ----\n",
    "class SentimentRegressor(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regressor(x).squeeze(1)\n",
    "\n",
    "# ---- TRAINING LOOP ----\n",
    "def run_epoch(model, loader, optimizer, is_train=True):\n",
    "    model.train() if is_train else model.eval()\n",
    "    total_preds, total_labels = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    for clips, targets in tqdm(loader, leave=False):\n",
    "        clips, targets = clips.to(device), targets.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            features = video_mae(clips).last_hidden_state.mean(dim=1)\n",
    "            preds = model(features)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_preds.extend(preds.detach().cpu().numpy())\n",
    "        total_labels.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader), np.array(total_preds), np.array(total_labels)\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    def to_label(x):\n",
    "        return \"Negative\" if x < -0.3 else \"Positive\" if x > 0.3 else \"Neutral\"\n",
    "    preds_label = [to_label(p) for p in preds]\n",
    "    labels_label = [to_label(l) for l in labels]\n",
    "\n",
    "    macro_f1 = f1_score(labels_label, preds_label, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels_label, preds_label, average=\"micro\")\n",
    "    recall = recall_score(labels_label, preds_label, average=None, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    acc = accuracy_score(labels_label, preds_label)\n",
    "    return macro_f1, micro_f1, recall, acc\n",
    "\n",
    "# ---- MAIN ----\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_mae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "video_mae.eval()\n",
    "for param in video_mae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "full_dataset = VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "regressor = SentimentRegressor(feature_dim=768).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=2e-4)\n",
    "\n",
    "# ---- TRAIN ----\n",
    "best_macro_f1 = -np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    train_loss, _, _ = run_epoch(regressor, train_loader, optimizer, is_train=True)\n",
    "    val_loss, val_preds, val_labels = run_epoch(regressor, val_loader, optimizer, is_train=False)\n",
    "\n",
    "    macro_f1, micro_f1, recall, acc = evaluate(val_preds, val_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(regressor.state_dict(), \"best_regressor_MSE.pth\")\n",
    "        print(f\"âœ… Best model saved at epoch {epoch} with Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "# ---- EVALUATE TEST ----\n",
    "test_loss, test_preds, test_labels = run_epoch(regressor, test_loader, optimizer, is_train=False)\n",
    "macro_f1, micro_f1, recall, acc = evaluate(test_preds, test_labels)\n",
    "print(\"\\n----- TEST RESULTS -----\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
